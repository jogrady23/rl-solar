{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d6b28e-ab99-47fe-9e08-0fe832fb4677",
   "metadata": {},
   "source": [
    "# Softmax Actor Critic with Avg Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f57b0-3416-49f0-941e-564df626956c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938c4bc-21e7-4c41-91ba-1298c6e6ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ddc77-1658-48a7-bc1d-f92e1e38799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "ARRAY_DIMENSION_TUPLE = (37,37)\n",
    "MOTOR_ANGLE_POWER_DRAW = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff225-628b-46fe-9ca1-26effd517cc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### General Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a0205-596c-42c0-a3b6-8afabe1cd3f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b46c3b-db28-4915-ae4d-97ad707df240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_data(data_path):\n",
    "    raw_df = pd.read_csv(data_path)\n",
    "    data_df = raw_df.copy()\n",
    "    # Make current positive\n",
    "    data_df = data_df.drop_duplicates(subset=['motor_1_position','motor_2_position'])\n",
    "    data_df['I_ivp_1'] = data_df['I_ivp_1'].abs()\n",
    "    data_df['power'] = data_df['I_ivp_1'] * data_df['V_ivp_1']\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b14ffe-987c-42e4-b03c-e1d48e0d935a",
   "metadata": {},
   "source": [
    "#### Index to motor position conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a872e-633f-4d7f-a911-8fa0ab392a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_motor_positions_to_2d_index(position_tuple):\n",
    "    # position tuple is (m1 position, m2 position)\n",
    "    return (int(position_tuple[0]//5), int(position_tuple[1]//5))\n",
    "\n",
    "def convert_2d_index_to_motor_positions(index_tuple):\n",
    "    return (index_tuple[0]*5, index_tuple[1]*5)\n",
    "\n",
    "def convert_1d_index_to_2d_index(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.unravel_index(index, dimensions)\n",
    "\n",
    "def convert_2d_index_to_1d_index(index_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return np.ravel_multi_index(index_tuple, dimensions)\n",
    "\n",
    "def convert_motor_positions_to_1d_index(position_tuple, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_1d_index(convert_motor_positions_to_2d_index(position_tuple), dimensions)\n",
    "\n",
    "def convert_1d_index_to_motor_positions(index, dimensions=ARRAY_DIMENSION_TUPLE):\n",
    "    return convert_2d_index_to_motor_positions(convert_1d_index_to_2d_index(index, dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd95a6-c7cd-4289-a462-10056a20e5bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Policy helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbef16a-77bb-47d7-99fe-44e36c1b2cd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1839ace-528a-400e-a5c7-cbc2cc4d16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prob(actor_av_array, temperature=1):\n",
    "    # Divide all values by temperature\n",
    "    temperature_array = actor_av_array/temperature\n",
    "    \n",
    "    # Find max state value\n",
    "    max_value = np.max(temperature_array)\n",
    "    \n",
    "    # Generate the numerator for each element by subtracting max value and exponentiating\n",
    "    numerator_array = temperature_array - max_value\n",
    "    numerator_array = np.exp(numerator_array)\n",
    "\n",
    "    # Get the denominator by summing all values in the numerator\n",
    "    denominator_array = np.sum(numerator_array)\n",
    "    \n",
    "    \n",
    "    # Calculate the softmax value array and return to agent\n",
    "    softmax_array = numerator_array / denominator_array\n",
    "    \n",
    "    return softmax_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710ccbe-0f6a-4d6b-9497-5455942a158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg_calc(value, last_value, window):\n",
    "    return (1/window)*value + (1-(1/window))*last_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748f703-5499-4dcb-8843-f213e728d516",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769522c-bc81-4f81-9618-d8337978124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarEnv:\n",
    "    def __init__(self, reward_data_path, shape=(37,37), motor_power_draw_per_degree=MOTOR_ANGLE_POWER_DRAW, control=None):\n",
    "        self.shape = shape\n",
    "        self.reward_array = np.zeros(shape)\n",
    "        self.last_motor_position = (90,90)\n",
    "        self.motor_power_draw_per_degree = motor_power_draw_per_degree\n",
    "        # load in reward data\n",
    "        rewards = load_and_prep_data(reward_data_path)\n",
    "        for index, row in rewards.iterrows():\n",
    "            motor_1_index = int(row['motor_1_position'].item()//5)\n",
    "            motor_2_index = int(row['motor_2_position'].item()//5)\n",
    "            position_reward = row['power'].item()\n",
    "            self.reward_array[motor_1_index][motor_2_index] = position_reward\n",
    "        if control is not None:\n",
    "            # Control sets the entire env value array to a constant for debugging\n",
    "            self.reward_array = np.full(shape, control)\n",
    "    \n",
    "    # For operation\n",
    "    def calculate_motor_power_draw(self, new_position_tuple):\n",
    "        # new_position_tuple is the new motor position action requested by the agent\n",
    "        return self.motor_power_draw_per_degree * (abs(self.last_motor_position[0] - new_position_tuple[0]) + \\\n",
    "                                              abs(self.last_motor_position[1] - new_position_tuple[1]))\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        # Take a motor position and return the reward, next motor positions\n",
    "        # action is motor positions\n",
    "        index_tuples = convert_motor_positions_to_2d_index(action)\n",
    "        power_gained = self.reward_array[index_tuples[0]][index_tuples[1]]\n",
    "        power_used = self.calculate_motor_power_draw(new_position_tuple=action)\n",
    "        next_state = convert_2d_index_to_motor_positions(index_tuples)\n",
    "        self.last_motor_position = next_state\n",
    "        return power_gained - power_used, next_state\n",
    "    \n",
    "    # For debugging\n",
    "    def get_reward_array(self):\n",
    "        return self.reward_array\n",
    "    \n",
    "    def get_env_shape(self):\n",
    "        return self.reward_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec059f9e-08a3-4211-8617-0a2285fc9384",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680ef40-22fa-445b-ad71-31b7c2a07567",
   "metadata": {},
   "source": [
    "The Agent is now a Softmax Actor Critic\n",
    "\n",
    "**Error**\n",
    "\n",
    "$$\\delta _{t} = R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}) - \\hat{v}(S_{t})$$\n",
    "\n",
    "**Avg Reward Update**\n",
    "\n",
    "$$\\bar{R} \\leftarrow \\bar{R} + \\alpha _{\\bar{R}} \\delta _{t}$$\n",
    "\n",
    "**Critic Update**\n",
    "\n",
    "$$\\hat{v}(S_{t},A_{t}) \\leftarrow \\hat{v}(S_{t},A_{t}) + \\alpha _{critic} \\delta _{t}$$\n",
    "\n",
    "**Actor Update**\n",
    "\n",
    "$$\\vec{\\theta}_{S_{t}} \\leftarrow \\vec{\\theta}_{S_{t}} + \\alpha_{\\theta} \\delta_{t}  [\\vec{x}_h(S_t,A_t) - \\vec{x}_{softmax_{S_t}}]$$\n",
    "\n",
    "**Reward Function**\n",
    "\n",
    "$$R = P_{solar} \\, - \\, P_{motor\\,draw}$$\n",
    "\n",
    "**Agent Parameters**\n",
    "\n",
    "* average reward step size: 0.001\n",
    "* actor step size: 0.1\n",
    "* critic step size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469e872-b860-43ca-858f-6c185cb07c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarAgent:\n",
    "    def __init__(self, actor_step_size, critic_step_size, avg_reward_step_size, temperature_value, solar_env, rolling_window, random_seed=RANDOM_SEED):\n",
    "        # Set the env\n",
    "        self.env = solar_env\n",
    "        \n",
    "        # Set step sizes\n",
    "        self.actor_step_size = actor_step_size\n",
    "        self.critic_step_size = critic_step_size\n",
    "        self.avg_reward_step_size = avg_reward_step_size\n",
    "        self.temperature = temperature_value\n",
    "        \n",
    "        # Set up memory for the actor and critic\n",
    "        self.env_shape = self.env.get_env_shape()\n",
    "        max_index_2d = self.env_shape[0] - 1\n",
    "        max_index_1d = convert_2d_index_to_1d_index((max_index_2d, max_index_2d))\n",
    "        self.agent_shape = (max_index_1d + 1, max_index_1d + 1)\n",
    "        self.critic_array = np.zeros(self.agent_shape)\n",
    "        self.actor_array = np.zeros(self.agent_shape)\n",
    "        self.actions_vector = np.array(range(0, max_index_1d + 1))\n",
    "        self.base_feature_vector = np.zeros(max_index_1d + 1)\n",
    "        \n",
    "        # Set up fields for agent steps\n",
    "        # Internally, the agent only operates in 1d index. All env comm is done via motor angles\n",
    "        self.random_generator = np.random.RandomState(random_seed) \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.state = None\n",
    "        self.last_reward = None\n",
    "        self.avg_reward = 0\n",
    "        self.step_softmax_prob = None\n",
    "    \n",
    "        # Set up tracking metric items\n",
    "        self.state_visits = np.zeros(self.env_shape) # track in 2d to better map to env map\n",
    "        self.total_energy = 0\n",
    "        self.rolling_power = 0\n",
    "        self.rolling_window = rolling_window\n",
    "        self.transition_dict = None\n",
    "    \n",
    "    # Agent Operation\n",
    "    # =============================================\n",
    "    \n",
    "    def agent_start(self):\n",
    "        # Initialize the agent\n",
    "        self.avg_reward = 0\n",
    "        self.last_state = convert_motor_positions_to_1d_index([90,90])\n",
    "        self.last_action = self.last_state\n",
    "        self.last_reward = 0\n",
    "        \n",
    "        # For tracking\n",
    "        self.state_visits[convert_1d_index_to_2d_index(self.last_state)] += 1\n",
    "    \n",
    "    \n",
    "    def agent_policy(self, state):\n",
    "        # Compute the softmax prob for actions in given state\n",
    "        softmax_prob_array = softmax_prob(self.actor_array[state], self.temperature)\n",
    "        \n",
    "        # Overlay the softmax probs onto actions vector\n",
    "        chosen_action = self.random_generator.choice(self.actions_vector, p=softmax_prob_array)\n",
    "        \n",
    "        # save softmax_prob as it will be useful later when updating the Actor\n",
    "        self.step_softmax_prob = softmax_prob_array.copy()\n",
    "        \n",
    "        # Return the 1d index of action\n",
    "        return chosen_action\n",
    "    \n",
    "    def agent_step(self):\n",
    "        # Make a policy decision\n",
    "        action = self.agent_policy(self.last_state)\n",
    "        \n",
    "        # FOR DEBUGGGING\n",
    "        # action = self.agent_policy(state_override)\n",
    "        # self.last_state = 0\n",
    "        # =================\n",
    "        \n",
    "        # Interact with the environment\n",
    "        reward, next_state_motor_pos = self.env.env_step(convert_1d_index_to_motor_positions(action))\n",
    "        next_state = convert_motor_positions_to_1d_index(next_state_motor_pos)\n",
    "        \n",
    "        # FOR DEBUGGING\n",
    "        # if self.last_state == 50 and next_state ==1203:\n",
    "        # # if self.last_state == 116 and next_state ==839:\n",
    "        #     print('reward', reward)\n",
    "        #     print('avg_reward', self.avg_reward)\n",
    "        #     print('next state value', np.sum(self.critic_array[next_state]))\n",
    "        #     print('last state value', np.sum(self.critic_array[self.last_state]))\n",
    "        #     print('delta', reward - self.avg_reward + np.sum(self.critic_array[next_state]) - np.sum(self.critic_array[self.last_state]))\n",
    "        #     print()\n",
    "        # ==================\n",
    "        \n",
    "        # Compute delta\n",
    "        # delta = reward - self.avg_reward + np.sum(self.critic_array[next_state]) - np.sum(self.critic_array[self.last_state])\n",
    "        delta = reward - self.avg_reward + np.mean(self.critic_array[next_state]) - np.mean(self.critic_array[self.last_state])\n",
    "\n",
    "\n",
    "        \n",
    "        # FOR DEBUGGING\n",
    "        # self.transition_dict = {}\n",
    "        # self.transition_dict['delta'] = delta\n",
    "        # self.transition_dict['reward'] = reward\n",
    "        # self.transition_dict['last_avg_reward'] = self.avg_reward\n",
    "        # self.transition_dict['last_critic_array'] = self.critic_array.copy()\n",
    "        # self.transition_dict['last_state'] = self.last_state\n",
    "        # self.transition_dict['next_state'] = next_state\n",
    "        # self.transition_dict['last_actor_array'] = self.actor_array.copy()\n",
    "        # self.transition_dict['softmax_prob'] = self.step_softmax_prob.copy()\n",
    "        # ==================\n",
    "        \n",
    "        # Update avg reward\n",
    "        self.avg_reward += self.avg_reward_step_size * delta\n",
    "        \n",
    "        # Update critic weights\n",
    "        self.critic_array[self.last_state][action] += self.critic_step_size * delta\n",
    "        \n",
    "        # Update actor weights\n",
    "        feature_vector = self.base_feature_vector.copy() # copy the zeros vector\n",
    "        feature_vector[self.last_state] = 1 # set last action to one\n",
    "        self.actor_array[self.last_state] += self.actor_step_size * delta * (feature_vector - self.step_softmax_prob)\n",
    "        \n",
    "        # Update last state, etc\n",
    "        self.last_state = next_state\n",
    "        \n",
    "        # FOR DEBUGGING\n",
    "        # self.transition_dict['new_avg_reward'] = self.avg_reward\n",
    "        # self.transition_dict['new_critic_array'] = self.critic_array.copy()\n",
    "        # self.transition_dict['new_actor_array'] = self.actor_array.copy()\n",
    "        # ==================\n",
    "        \n",
    "        # For debugging\n",
    "        # self.transition_dict = {\n",
    "        #     'reward': reward,\n",
    "        #     'avg_reward': self.avg_reward,\n",
    "        #     'actor_array': self.actor_array,\n",
    "        #     'critic_array': self.critic_array,\n",
    "        #     'delta': delta,\n",
    "        #     'action': action,\n",
    "        #     'last_state': \n",
    "        # }\n",
    "        \n",
    "        # For tracking\n",
    "        self.total_energy += reward\n",
    "        self.rolling_power = rolling_avg_calc(reward, self.rolling_power, self.rolling_window)\n",
    "        self.state_visits[convert_1d_index_to_2d_index(self.last_state)] += 1\n",
    "    \n",
    "    # Tracking\n",
    "    # =============================================\n",
    "    \n",
    "    def get_critic_array(self):\n",
    "        return self.critic_array.copy()\n",
    "    \n",
    "    def get_actor_array(self):\n",
    "        return self.actor_array.copy()\n",
    "    \n",
    "    def set_actor_array(self, array):\n",
    "        self.actor_array = array\n",
    "        \n",
    "    def get_actions_vector(self):\n",
    "        return self.actions_vector\n",
    "    \n",
    "    def get_base_feature_vector(self):\n",
    "        return self.base_feature_vector\n",
    "    \n",
    "    def get_agent_energy(self):\n",
    "        return self.total_energy\n",
    "    \n",
    "    def get_agent_rolling_power(self):\n",
    "        return self.rolling_power\n",
    "    \n",
    "    def get_transition_dict(self):\n",
    "        return self.transition_dict\n",
    "    \n",
    "    def get_state_visits(self):\n",
    "        return self.state_visits.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d30d9-816c-4b97-a8f7-6e1817d70857",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c9354-8c79-49f3-8b0b-e0ac64cdc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_error(agent_array, env_array):\n",
    "    return np.sqrt((agent_array - env_array)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b185c2-663c-4d9a-bc98-6d85460ffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_dict_to_df(progress_dict):\n",
    "    dict_list = []\n",
    "    for x in progress_dict.keys():\n",
    "        temp_dict = progress_dict[x]\n",
    "        temp_dict['step'] = x\n",
    "        dict_list.append(temp_dict)\n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63427c-3527-4cd8-9051-195a85ebdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_experiment(exp_agent, exp_env, steps, interval):\n",
    "    progress_dict = {}\n",
    "    exp_agent.agent_start()\n",
    "    progress_dict['0'] = {\n",
    "                'state_value': exp_agent.get_critic_array(),\n",
    "                'action_prob': exp_agent.get_actor_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    for i in tqdm(range(1, steps + 1)):\n",
    "    # for i in range(1, steps + 1): # no TQDM option\n",
    "        exp_agent.agent_step()\n",
    "        if i%interval == 0:\n",
    "            progress_dict[str(i)] = {\n",
    "                'state_value': exp_agent.get_critic_array(),\n",
    "                'action_prob': exp_agent.get_actor_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    progress_df = progress_dict_to_df(progress_dict)\n",
    "    return exp_agent, progress_dict, progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dff5d-d828-495d-b50d-a9a3ae1a03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_power(progress_df, exp_env, height, width):\n",
    "    max_output = exp_env.get_reward_array().max()\n",
    "    progress_df['env_max'] = max_output\n",
    "    progress_df['optimal_energy'] = progress_df['step'].astype(float) * max_output\n",
    "    progress_df['difference'] = (progress_df['rolling_power'] - progress_df['env_max']) / progress_df['env_max']\n",
    "    make_subplots_plot(df=progress_df, x='step', subplot_group_list=[\n",
    "    {\n",
    "        'title': 'Reward Comparison (Agent vs Max)',\n",
    "         'columns': ['env_max','rolling_power']\n",
    "    },\n",
    "        {\n",
    "        'title': 'Energy Comparison (Agent vs Max)',\n",
    "         'columns': ['total_energy', 'optimal_energy']\n",
    "        }\n",
    "    ], height=height, width=width\n",
    "                 )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8327a61-ebab-4215-b354-77151b048ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subplots_plot(df, x, subplot_group_list, height=400, width=400, plot_title=''):\n",
    "    fig = make_subplots(rows=len(subplot_group_list), cols=1,\n",
    "                       subplot_titles=[x['title'] for x in subplot_group_list])\n",
    "    \n",
    "    for i in range(len(subplot_group_list)):\n",
    "        row = i+1\n",
    "        title = subplot_group_list[i]['title']\n",
    "        for column_name in subplot_group_list[i]['columns']:\n",
    "            fig.append_trace(go.Scatter(\n",
    "                x=df[x],\n",
    "                y=df[column_name], name=column_name\n",
    "            ), row=row, col=1)\n",
    "\n",
    "    fig.update_layout(height=height, width=width, title_text=plot_title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0ed7f-fca1-4a86-90da-c2d4a8b76fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_array_evolution(exp_progress_dict, exp_interval, field, width_plot, height_plot, zmax=None, zmin=None):\n",
    "    matrix_list = [exp_progress_dict[x][field] for x in exp_progress_dict.keys()]\n",
    "    fig = go.Figure(\n",
    "        data=[go.Heatmap(z=matrix_list[0], zmax=zmax, zmin=zmin)],layout=go.Layout(\n",
    "                title=\"Step 0\",\n",
    "                updatemenus=[dict(\n",
    "                    type=\"buttons\",\n",
    "                    buttons=[dict(label=\"Play\",\n",
    "                                  method=\"animate\",\n",
    "                                  args=[None]),\n",
    "                            dict(label=\"Pause\",\n",
    "                                 method=\"animate\",\n",
    "                                 args=[None,\n",
    "                                       {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                        \"mode\": \"immediate\",\n",
    "                                        \"transition\": {\"duration\": 0}}],\n",
    "                                 )])]\n",
    "            ),\n",
    "            frames=[go.Frame(data=[go.Heatmap(z=matrix_list[i])], \n",
    "                             layout=go.Layout(title_text=f\"Step {i * exp_interval}\")) for i in range(1, len(matrix_list))]\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.update_layout({\n",
    "        'height': height_plot,\n",
    "        'width': width_plot}\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8db1f1-eb36-48c2-8528-f146169dbd1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Validate Agent Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659ce2e-da1b-42a0-84d0-1ae7e6edf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment factors\n",
    "exp_env_control = None\n",
    "exp_data_path = '../../../rl_agent/simulation_data/data/corrected_motors/run_5_kitchen_no_lights.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59270191-0081-447e-b68c-80c6e3d5f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set runtime values\n",
    "steps = 100000\n",
    "interval = 1000\n",
    "\n",
    "# Agent factors\n",
    "exp_actor_step_size = 0.1\n",
    "exp_temperature = 0.01\n",
    "exp_critic_step_size = 1\n",
    "exp_avg_reward_step_size = 0.01\n",
    "exp_rolling_avg_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a67299-2264-4a6e-bab9-1fc64e0da7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "experiment_env = SolarEnv(reward_data_path=exp_data_path, shape=(37,37), control=exp_env_control)\n",
    "# Create agent\n",
    "experiment_agent = SolarAgent(actor_step_size=exp_actor_step_size, critic_step_size=exp_critic_step_size, \n",
    "                              avg_reward_step_size=exp_avg_reward_step_size, temperature_value=exp_temperature, \n",
    "                              solar_env=experiment_env, rolling_window=exp_rolling_avg_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a83df-dcfa-48a3-908d-a94ea82eb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_agent.agent_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e01f0c-ceff-466b-8c5f-79758dba12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(1, 100000)):\n",
    "    experiment_agent.agent_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b77ea-bd8f-4af9-a5ce-76b15abb44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_df = pd.DataFrame(softmax_prob(experiment_agent.get_actor_array()[394], temperature=0.00007)).reset_index().rename(columns={'index':'action',0:'prob'})\n",
    "critic_df = pd.DataFrame(experiment_agent.get_critic_array()[394]).reset_index().rename(columns={'index':'action',0:'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c913b02-0bf7-4e24-97a6-ee7c985999e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe0ff9-b965-4514-9f94-b424c0e3efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = actor_df.merge(critic_df, on='action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89c067-f627-4766-b582-83043ddb1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_actor_critic_plot(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553df87d-a16d-4fff-b994-08fbc59507c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_actor_critic_plot(df):\n",
    "    fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=df['action'],\n",
    "        y=df['prob']\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=df['action'],\n",
    "        y=df['value']\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(height=600, width=600, title_text=\"Value Comparisons\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e632eef-ffd2-49d1-a18c-6e15325a3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_transition = experiment_agent.get_transition_dict()\n",
    "print(last_transition['new_actor_array'][0][1368])\n",
    "print(np.reshape(last_transition['new_actor_array'][0] - last_transition['last_actor_array'][0], (37,37)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a7f8f-ecc3-444f-b8a2-c8ef89d13191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25902fe5-d07e-4eff-b22d-307a3e61dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.ones((1369,1369))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324fb09-5358-4065-b73a-4308d212c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array[0][1] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4532f-f874-4e98-b92b-2b7d1b233e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590329ce-b247-42fe-9643-c934a04ce4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_agent.set_actor_array(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a3ab1-0bde-4b4d-b77c-cf1c9ff4af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_agent.agent_policy(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46258b2-b13c-41dd-9e5a-5ca28d90076c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resume here with experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357ddf9-2eb9-4c27-aa7e-856c90a0544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting max/min\n",
    "zmax_plot = 0.015\n",
    "zmin_plot = 0\n",
    "width_plot = 400\n",
    "height_plot = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e131d-83fa-40eb-9f8e-f26930b80759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment factors\n",
    "exp_env_control = None\n",
    "exp_data_path = '../../../rl_agent/simulation_data/data/corrected_motors/run_5_kitchen_no_lights.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb59999-2d94-4357-9c80-ccd6b6a36e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set runtime values\n",
    "steps = 100000\n",
    "interval = 100\n",
    "\n",
    "# Agent factors\n",
    "exp_actor_step_size = 0.01\n",
    "exp_temperature = 0.00001\n",
    "exp_critic_step_size = 0.05\n",
    "exp_avg_reward_step_size = 0.01\n",
    "exp_rolling_avg_steps = 10\n",
    "exp_random_seed = RANDOM_SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4af96-213b-4bb9-886f-c770884beeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "experiment_env = SolarEnv(reward_data_path=exp_data_path, shape=(37,37), control=exp_env_control)\n",
    "# Create agent\n",
    "experiment_agent = SolarAgent(actor_step_size=exp_actor_step_size, critic_step_size=exp_critic_step_size, \n",
    "                              avg_reward_step_size=exp_avg_reward_step_size, temperature_value=exp_temperature, \n",
    "                              solar_env=experiment_env, rolling_window=exp_rolling_avg_steps, random_seed=exp_random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf2922-97d1-475b-8339-96c152178b4c",
   "metadata": {},
   "source": [
    "#### Initiate an experiment\n",
    "Remember to re-initialize the agent above or else it will resume learning on the exisitng agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56169d-6131-46c9-b9b3-a790b79bd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_agent, progress_dict, progress_df = run_agent_experiment(exp_agent=experiment_agent, \n",
    "                                                                   exp_env=experiment_env, steps=steps, interval=interval)\n",
    "plot_rolling_power(progress_df, experiment_env, height=600, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7d3ee-fae0-46bb-9cae-858cc6f914fb",
   "metadata": {},
   "source": [
    "#### Show the agent difference from known optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b70c4-a14e-4f28-b3c7-8d3d764760f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Misc Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1fb92-7490-4faf-81fa-bacd338a98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_agent.get_transition_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b8def-7e54-4ae3-bbb6-5357746bfccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_agent.get_critic_array()[116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e43a2-5ac4-4cf7-8c77-8878e0939ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1369):\n",
    "    if np.isnan(softmax_prob(experiment_agent.get_actor_array()[i], 1)[0]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20a67a-b0ef-4260-b7f9-c201b3d12699",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_agent.get_actor_array()[116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28c791-a590-4b9c-8f2a-d620f52aa779",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_prob(experiment_agent.get_actor_array()[116], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd5caa-8617-4d10-9725-f3c85b5db026",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e089d-ba49-432f-a365-8032603f1df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_rolling_power(progress_df, experiment_env, height=600, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d1857-2d03-4bb9-adf8-6e9b43e14862",
   "metadata": {},
   "source": [
    "#### Visualize state visits and learned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ea419-aa62-4162-9975-ac1f318c2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_max_step = False\n",
    "specific_step = '5000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8150cc-903e-4eba-bd7e-6d86af1c8312",
   "metadata": {},
   "source": [
    "State visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca816f-012b-4e4c-8c7f-4f0b0417b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_visits'], width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ff4a1-fa21-475c-86c1-0ca6c6b971e9",
   "metadata": {},
   "source": [
    "State values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e90fa4-dccd-43d7-8b96-e8a54d2a7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convert_2d_index_to_1d_index((10,24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0630444-7a92-405c-9acc-6d8dab7e2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_value'], width=700, \n",
    "          height=700, zmin=zmin_plot, zmax=zmax_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14214779-2fbe-4ef6-a544-07b8b5838b39",
   "metadata": {},
   "source": [
    "**Action Probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032c0a7-f7e3-44ba-9ea1-114c3c638711",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['action_prob'], width=700, \n",
    "          height=700, zmin=None, zmax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0945d39-f673-4897-97c2-aa2fdb125d2a",
   "metadata": {},
   "source": [
    "#### Environment True Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c2d0f-778b-4b03-9983-0d22fadb977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(experiment_env.get_reward_array(), width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eaa99-d91e-4ca2-a5cc-1c0e5470661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_visits', width_plot, height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ed37e-28a0-472b-a036-714c7dc1a198",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visulaize evolution of learned state value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6398b-6be2-4f06-839c-d426b342c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_value', width_plot, height_plot, zmax_plot, zmin_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3221bf-843c-4766-b827-1682a3bf7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE vs steps\n",
    "px.scatter(progress_df, x='step', y='rolling_power', width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72827c36-139e-4032-8eab-65d772cb6d04",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549b6ef-2bb5-469e-98e6-5b7dbdb0682a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Older Arduino Functions to Scan Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff4000-d540-4893-a785-11236d03159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request codes\n",
    "MOTOR_CONTROL = 1000\n",
    "STATE_REQUEST = 2000\n",
    "RESET_CODE = 6666\n",
    "\n",
    "def scan_space(arduino):\n",
    "    # Run start\n",
    "    run_start = time.time()\n",
    "    data_dict_list = []\n",
    "    last_motor_interval = 0\n",
    "    last_measure_interval = -1\n",
    "    motor_frequency = 2\n",
    "    measure_frequency = 1\n",
    "    # Set timeouts\n",
    "    abort = False\n",
    "    \n",
    "    for xy_degree in range(0, 181, 5):\n",
    "        for yz_degree in range(0, 181, 5):\n",
    "            si.write_serial_line(arduino, [MOTOR_CONTROL, xy_degree, yz_degree], print_message=False)\n",
    "            new_message, abort = si.listen_for_serial(arduino)\n",
    "            if new_message is not None and not abort:\n",
    "                data_dict_list.append(new_message)\n",
    "            elif abort:\n",
    "                break\n",
    "            else:\n",
    "                print('Empty message received without abort issue')\n",
    "            time.sleep(0.1) # Wait for steady state\n",
    "        if abort:\n",
    "            break\n",
    "        print('xy:',xy_degree,'yz:',yz_degree)\n",
    "    # Write back to start state\n",
    "    write_serial_line(arduino, [si.MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    return pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07994661-9e43-4372-b543-b2641c5cad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('\\nARDUINO CONTROL TESTING')\n",
    "    print('-------------------------')\n",
    "    # Initialize serial port\n",
    "    print('\\nIniitalizing device...')\n",
    "    serial_port = '/dev/cu.usbmodem14101'\n",
    "    baud_rate = 9600\n",
    "    timeout = 5\n",
    "    arduino = si.initialize_serial(serial_port=serial_port, baud_rate=baud_rate, timeout=timeout)\n",
    "    print('\\t - SUCCESS: Device initialized.')\n",
    "    \n",
    "    si.write_serial_line(arduino, [MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    # Run a loop where motor position incremented every 5 seconds, print out message\n",
    "    print('\\nBeginning loop sequence...')\n",
    "#     data = scan_space(arduino)\n",
    "    print('\\t - Loop complete.')\n",
    "\n",
    "    # Add relative time to returned data and print out\n",
    "#     data['t_relative'] = data['timestamp'] - data['timestamp'].iloc[0]\n",
    "    print('\\nData broadcasted by Arduino:\\n')\n",
    "    \n",
    "#     data.to_csv('/Users/jackogrady/Git/rl-solar/rl_agent/simulation_data/data/run_6_kitchen_no_lights_swapped_motors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55694cd-2430-4233-943d-51758fc16cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1b648-e1a8-4d7d-84ca-56d3c3b878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_serial_line(arduino, [1000, 180, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e08c6-02c9-4216-80d9-4968f08b292b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
