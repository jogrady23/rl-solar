{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d6b28e-ab99-47fe-9e08-0fe832fb4677",
   "metadata": {},
   "source": [
    "# Softmax Actor Critic with Avg Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f57b0-3416-49f0-941e-564df626956c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f938c4bc-21e7-4c41-91ba-1298c6e6ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "837ddc77-1658-48a7-bc1d-f92e1e38799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22033ac-a176-4e24-a1b0-ee8ff32f59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 14)\n"
     ]
    }
   ],
   "source": [
    "print(np.unravel_index(125, (37,37)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff225-628b-46fe-9ca1-26effd517cc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### General Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a0205-596c-42c0-a3b6-8afabe1cd3f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b46c3b-db28-4915-ae4d-97ad707df240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_data(data_path):\n",
    "    raw_df = pd.read_csv(data_path)\n",
    "    data_df = raw_df.copy()\n",
    "    # Make current positive\n",
    "    data_df = data_df.drop_duplicates(subset=['motor_1_position','motor_2_position'])\n",
    "    data_df['I_ivp_1'] = data_df['I_ivp_1'].abs()\n",
    "    data_df['power'] = data_df['I_ivp_1'] * data_df['V_ivp_1']\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949a872e-633f-4d7f-a911-8fa0ab392a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_motor_positions_to_index(position_tuple):\n",
    "    # position tuple is (m1 position, m2 position)\n",
    "    return (int(position_tuple[0]//5), int(position_tuple[1]//5))\n",
    "\n",
    "def convert_index_to_motor_positions(index_tuple):\n",
    "    return (index_tuple[0]*5, index_tuple[1]*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd95a6-c7cd-4289-a462-10056a20e5bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Policy helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbef16a-77bb-47d7-99fe-44e36c1b2cd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1839ace-528a-400e-a5c7-cbc2cc4d16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prob(actor_av_array, temperature=1):\n",
    "    # Divide all values by temperature\n",
    "    temperature_array = actor_av_array/temperature\n",
    "    \n",
    "    # Find max state value\n",
    "    max_value = np.max(actor_av_array)\n",
    "    \n",
    "    # Generate the numerator for each element by subtracting max value and exponentiating\n",
    "    numerator_array = actor_av_array - max_value\n",
    "    numerator_array = np.exp(numerator_array)\n",
    "\n",
    "    # Get the denominator by summing all values in the numerator\n",
    "    denominator_array = np.sum(numerator_array)\n",
    "    \n",
    "    \n",
    "    # Calculate the softmax value array and return to agent\n",
    "    softmax_array = numerator_array / denominator_array\n",
    "    \n",
    "    return softmax_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4710ccbe-0f6a-4d6b-9497-5455942a158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg_calc(value, last_value, window):\n",
    "    return (1/window)*value + (1-(1/window))*last_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a547fc-b360-4843-92ff-03bcb27c9536",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261d6701-7083-412c-86c4-22e30df7349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_thing_tanh(initial_value, chi, reward, steps):\n",
    "    last_value = initial_value\n",
    "    for i in range(steps):\n",
    "        update = last_value + 0.1*(reward - last_value) * (1 + np.tanh(np.abs((reward - last_value)/last_value)))**chi\n",
    "        last_value = update\n",
    "        print(round(update,4))\n",
    "        \n",
    "def simulate_thing_no_scaling(initial_value, chi, reward, steps):\n",
    "    last_value = initial_value\n",
    "    for i in range(steps):\n",
    "        update = last_value + 0.1*(reward - last_value)\n",
    "        last_value = update\n",
    "        print(round(update,4))\n",
    "        \n",
    "def simulate_thing_notanh(initial_value, chi, reward, steps):\n",
    "    last_value = initial_value\n",
    "    for i in range(steps):\n",
    "        update = last_value + 0.1*(reward - last_value) * (1 + np.abs((reward - last_value)/last_value))**chi\n",
    "        last_value = update\n",
    "        print(round(update,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748f703-5499-4dcb-8843-f213e728d516",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d769522c-bc81-4f81-9618-d8337978124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarEnv:\n",
    "    def __init__(self, reward_data_path, shape=(37,37), control=None):\n",
    "        self.shape = shape\n",
    "        self.reward_array = np.zeros(shape)\n",
    "        # load in reward data\n",
    "        rewards = load_and_prep_data(reward_data_path)\n",
    "        for index, row in rewards.iterrows():\n",
    "            motor_1_index = int(row['motor_1_position'].item()//5)\n",
    "            motor_2_index = int(row['motor_2_position'].item()//5)\n",
    "            position_reward = row['power'].item()\n",
    "            self.reward_array[motor_1_index][motor_2_index] = position_reward\n",
    "        if control is not None:\n",
    "            self.reward_array = np.full(shape, control)\n",
    "    \n",
    "    # For debugging\n",
    "    def get_reward_array(self):\n",
    "        return self.reward_array\n",
    "    \n",
    "    def get_env_shape(self):\n",
    "        return self.reward_array.shape\n",
    "                                          \n",
    "    # Not needed right now\n",
    "    def env_init(self):\n",
    "        \"\"\"\n",
    "        Setup for the environment called when the experiment first starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # Not needed right now\n",
    "    def env_start(self):\n",
    "        \"\"\"\n",
    "        The first method called when the experiment starts, called before the\n",
    "        agent starts.\n",
    "\n",
    "        Returns:\n",
    "            The first state from the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \"\"\"A step taken by the environment.\n",
    "\n",
    "        Args:\n",
    "            action: The action taken by the agent, a tuple of motor positions\n",
    "\n",
    "        Returns:\n",
    "            (float, state): a tuple of the reward, state\n",
    "        \"\"\"\n",
    "        index_tuples = convert_motor_positions_to_index(action)\n",
    "        return self.reward_array[index_tuples[0]][index_tuples[1]], convert_index_to_motor_positions(index_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac738823-8ec9-4565-b25a-cca2f587288d",
   "metadata": {},
   "source": [
    "Visualizing the reward array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec059f9e-08a3-4211-8617-0a2285fc9384",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680ef40-22fa-445b-ad71-31b7c2a07567",
   "metadata": {},
   "source": [
    "The Agent is now a Softmax Actor Critic\n",
    "\n",
    "**Error**\n",
    "\n",
    "delta<sub>t</sub> = R<sub>t+1</sub> - R̅ + v(s<sub>t+1</sub>) - v(s<sub>t</sub>)\n",
    "\n",
    "**Avg Reward Update**\n",
    "\n",
    "R̅ <-- R̅ + alpha<sub>R̅</sub> * delta\n",
    "\n",
    "**Critic Update**\n",
    "\n",
    "AV<sub>critic</sub> <-- AV<sub>critic</sub> + alpha<sub>critic</sub> * delta\n",
    "\n",
    "**Actor Update**\n",
    "\n",
    "AV<sub>actor</sub> <-- AV<sub>actor</sub> + alpha<sub>actor</sub> * delta * [(1 - softmax(a) if a=a<sub>chosen</sub> else (0 - softmax(a)]\n",
    "\n",
    "**Other Info**\n",
    "This uses both a network for the actor and the critic. The critic is learning actual state value functions with a TD-like update rule, \n",
    "whereas the actor network is learning policy values which affect the probabilities of actions being taken, not energy values.\n",
    "\n",
    "The step size of the actor should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "140a7f4b-2545-4876-8bdc-46ac35b77ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {\n",
    "    \"iht_size\": 4096,\n",
    "    \"num_tilings\": 8,\n",
    "    \"num_tiles\": 8,\n",
    "    \"actor_step_size\": 1e-1,\n",
    "    \"critic_step_size\": 1e-0,\n",
    "    \"avg_reward_step_size\": 1e-2,\n",
    "    \"num_actions\": 3,\n",
    "    \"seed\": 99,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdd9bb7-6b1e-479e-ad01-beea78f8b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index_array(shape):\n",
    "    index_pairs = []\n",
    "    for i in range(shape[0]):\n",
    "        row_list = []\n",
    "        for j in range(shape[1]):\n",
    "            row_list.append(str(i) + ',' + str(j))\n",
    "        index_pairs.append(row_list)\n",
    "    return np.array(index_pairs, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d248ab4-0092-4b21-8369-bf3b703c9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_index(str_index):\n",
    "    return [int(x) for x in str_index.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c33263e7-12cf-468c-b60a-69634e11c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array = np.zeros((36,36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06ce9829-ba1a-4d3c-8222-84a0668d6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array[0][0] = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3212408-080c-422a-962d-576b0acb41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_array[35][35] = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37e5f01f-dbe4-4f4c-a448-e58a3a8ca87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.array(generate_index_array((36,36)).flatten()), p=prob_array.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cda131d4-e6d6-4e57-94a1-849fc8b616f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '0,0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aac446e7-d8bb-4287-a428-6d62b015115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "print([int(x) for x in test.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7dd1a8-68c3-4da3-8161-1c2aab73c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(generate_index_array((36,36)), dtype=object).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1f588-638c-45b1-9817-f31faef9ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = np.meshgrid(np.arange(36), np.arange(36), indexing='ij')\n",
    "result = np.stack(rc, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b7a61-abff-4fcc-9d1b-c8013fe1bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469e872-b860-43ca-858f-6c185cb07c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarAgent:\n",
    "    def __init__(self, actor_step_size, critic_step_size, avg_reward_step_size, temperature_value, solar_env, random_seed=RANDOM_SEED):\n",
    "        # Set the env\n",
    "        self.env = solar_env\n",
    "        \n",
    "        # Set step sizes\n",
    "        self.actor_step_size = actor_step_size\n",
    "        self.critic_step_size = critic_step_size\n",
    "        self.avg_reward_step_size = avg_reward_step_size\n",
    "        self.temperature = temperature_value\n",
    "        \n",
    "        # Set up memory for the actor and critic\n",
    "        self.env_shape = self.env.get_env_shape()\n",
    "        self.critic_array = np.zeros(self.env_shape)\n",
    "        self.actor_array = np.zeros(self.env_shape)\n",
    "        self.actions_array = generate_index_array(self.env_shape)\n",
    "        \n",
    "        # Set up fields for agent steps\n",
    "        self.random_generator = np.random.RandomState(random_seed) \n",
    "        self.last_state = None\n",
    "        self.state = None\n",
    "        self.last_reward = None\n",
    "        self.avg_reward = 0\n",
    "        self.step_softmax_prob = None\n",
    "    \n",
    "        # Set up tracking metric items\n",
    "        self.state_visits = np.zeros(self.env_shape)\n",
    "        self.total_energy = 0\n",
    "        self.rolling_power = 0\n",
    "        self.rolling_window = rolling_window\n",
    "        self.transition_dict = None\n",
    "    \n",
    "    # Agent Operation\n",
    "    # =============================================\n",
    "    \n",
    "    def agent_start(self):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            self.last_action [int] : The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.avg_reward = 0\n",
    "        self.last_state = [90,90]\n",
    "        self.last_reward = 0\n",
    "        self.state, self.reward = self.env.env_step(self.last_state)\n",
    "    \n",
    "    def get_state_values(self, state):\n",
    "        converted_index = convert_motor_positions_to_index(state)\n",
    "        return self.state_values[converted_index[0]][converted_index[1]]\n",
    "    \n",
    "    def agent_policy(self):\n",
    "        softmax_prob_array = softmax_prob(self.actor_array, self.temperature)\n",
    "        \n",
    "        ### Resume here\n",
    "        # Trying to resolve ranom choice needing 1D vs 2D\n",
    "        chosen_action_str = self.random_generator.choice(self.actions_array.flatten(), p=softmax_prob_array.flatten())\n",
    "        # Format chosen action\n",
    "        chosen_action = convert_string_index(chosen_action_str)\n",
    "        \n",
    "        # save softmax_prob as it will be useful later when updating the Actor\n",
    "        self.step_softmax_prob = softmax_prob.copy()\n",
    "        \n",
    "        # On return, convert to motor positions\n",
    "        return convert_index_to_motor_positions(chosen_action)\n",
    "    \n",
    "    def agent_step(self):\n",
    "        # Make a policy decision\n",
    "        action = self.agent_policy()\n",
    "        \n",
    "        # Interact with the environment\n",
    "        reward, next_state = self.env.env_step(action)\n",
    "        \n",
    "        # Compute delta first\n",
    "        delta = reward - self.avg_reward + np.sum(self.critic_w[active_tiles]) - np.sum(self.critic_w[self.prev_tiles]\n",
    "        \n",
    "        \n",
    "        # TD Update\n",
    "        last_state_value = self.get_state_values(self.last_state)\n",
    "        new_state_value = self.get_state_values(next_state)\n",
    "        error_term = reward - last_state_value\n",
    "        scaling_term = (1 + np.tanh(np.abs(error_term/last_state_value)))**self.chi\n",
    "        last_state_index = convert_motor_positions_to_index(self.last_state)\n",
    "        self.state_values[last_state_index[0]][last_state_index[1]] = last_state_value + self.step_size * error_term * scaling_term\n",
    "        \n",
    "        # For debugging\n",
    "        self.transition_dict = {\n",
    "            'reward': reward,\n",
    "            'new_state_value': new_state_value,\n",
    "            'last_state_value': last_state_value,\n",
    "            'error_term': error_term,\n",
    "            'updated_value_estimate': last_state_value + self.step_size * error_term\n",
    "        }\n",
    "        \n",
    "        # Update internal variables\n",
    "        self.last_state = next_state\n",
    "        \n",
    "        # For tracking\n",
    "        self.total_energy += reward\n",
    "        self.rolling_power = rolling_avg_calc(reward, self.rolling_power, self.rolling_window)\n",
    "        self.state_visits[last_state_index[0]][last_state_index[1]] += 1\n",
    "        \n",
    "        # =================\n",
    "        angle, ang_vel = state\n",
    "\n",
    "        ### Use self.tc to get active_tiles using angle and ang_vel (1 line)\n",
    "        # active_tiles = ?    \n",
    "        # ----------------\n",
    "        # your code here\n",
    "        active_tiles = self.tc.get_tiles(angle, ang_vel)\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        ### Compute delta using Equation (1) (1 line)\n",
    "        # delta = ?\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        delta = reward - self.avg_reward + np.sum(self.critic_w[active_tiles]) - np.sum(self.critic_w[self.prev_tiles])\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        ### update average reward using Equation (2) (1 line)\n",
    "        # self.avg_reward += ?\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        self.avg_reward += self.avg_reward_step_size * delta\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        # update critic weights using Equation (3) and (5) (1 line)\n",
    "        # self.critic_w[self.prev_tiles] += ?\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        self.critic_w[self.prev_tiles] += self.critic_step_size * delta\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        # update actor weights using Equation (4) and (6)\n",
    "        # We use self.softmax_prob saved from the previous timestep\n",
    "        # We leave it as an exercise to verify that the code below corresponds to the equation.\n",
    "        for a in self.actions:\n",
    "            if a == self.last_action:\n",
    "                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (1 - self.softmax_prob[a])\n",
    "            else:\n",
    "                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (0 - self.softmax_prob[a])\n",
    "\n",
    "        ### set current_action by calling self.agent_policy with active_tiles (1 line)\n",
    "        # current_action = ? \n",
    "        # ----------------\n",
    "        # your code here\n",
    "        current_action = self.agent_policy(active_tiles)\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        self.prev_tiles = active_tiles\n",
    "        self.last_action = current_action\n",
    "\n",
    "        return self.last_action\n",
    "    \n",
    "    # Tracking\n",
    "    # =============================================\n",
    "    \n",
    "    def get_critic_array(self):\n",
    "        return self.critic_array.copy()\n",
    "    \n",
    "    def get_actor_array(self):\n",
    "        return self.actor_array.copy()\n",
    "    \n",
    "    def get_agent_energy(self):\n",
    "        return self.total_energy\n",
    "    \n",
    "    def get_agent_rolling_power(self):\n",
    "        return self.rolling_power\n",
    "    \n",
    "    def get_transition_dict(self):\n",
    "        return self.transition_dict\n",
    "    \n",
    "    def get_state_visits(self):\n",
    "        return self.state_visits.copy()\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d30d9-816c-4b97-a8f7-6e1817d70857",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c9354-8c79-49f3-8b0b-e0ac64cdc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_error(agent_array, env_array):\n",
    "    return np.sqrt((agent_array - env_array)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b185c2-663c-4d9a-bc98-6d85460ffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_dict_to_df(progress_dict):\n",
    "    dict_list = []\n",
    "    for x in progress_dict.keys():\n",
    "        temp_dict = progress_dict[x]\n",
    "        temp_dict['step'] = x\n",
    "        dict_list.append(temp_dict)\n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63427c-3527-4cd8-9051-195a85ebdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_experiment(exp_agent, exp_env, steps, interval):\n",
    "    progress_dict = {}\n",
    "    exp_agent.agent_start()\n",
    "    progress_dict['0'] = {\n",
    "                'state_value': exp_agent.get_state_value_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'mse': calculate_value_error(exp_agent.get_state_value_array(), exp_env.get_reward_array()),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    for i in tqdm(range(1, steps + 1)):\n",
    "        exp_agent.agent_step()\n",
    "        if i%interval == 0:\n",
    "            progress_dict[str(i)] = {\n",
    "                'state_value': exp_agent.get_state_value_array(),\n",
    "                'rolling_power': exp_agent.get_agent_rolling_power(),\n",
    "                'mse': calculate_value_error(exp_agent.get_state_value_array(), exp_env.get_reward_array()),\n",
    "                'state_visits': exp_agent.get_state_visits(),\n",
    "                'total_energy': exp_agent.get_agent_energy()\n",
    "            }\n",
    "    progress_df = progress_dict_to_df(progress_dict)\n",
    "    return exp_agent, progress_dict, progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dff5d-d828-495d-b50d-a9a3ae1a03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_power(progress_df, exp_env, height, width):\n",
    "    max_output = exp_env.get_reward_array().max()\n",
    "    progress_df['env_max'] = max_output\n",
    "    progress_df['optimal_energy'] = progress_df['step'].astype(float) * max_output\n",
    "    progress_df['difference'] = (progress_df['rolling_power'] - progress_df['env_max']) / progress_df['env_max']\n",
    "    make_subplots_plot(df=progress_df, x='step', subplot_group_list=[\n",
    "    {\n",
    "        'title': 'Reward Comparison (Agent vs Max)',\n",
    "         'columns': ['env_max','rolling_power']\n",
    "    },\n",
    "        {\n",
    "        'title': 'Energy Comparison (Agent vs Max)',\n",
    "         'columns': ['total_energy', 'optimal_energy']\n",
    "        }\n",
    "    ], height=height, width=width\n",
    "                 )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8327a61-ebab-4215-b354-77151b048ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subplots_plot(df, x, subplot_group_list, height=400, width=400, plot_title=''):\n",
    "    fig = make_subplots(rows=len(subplot_group_list), cols=1,\n",
    "                       subplot_titles=[x['title'] for x in subplot_group_list])\n",
    "    \n",
    "    for i in range(len(subplot_group_list)):\n",
    "        row = i+1\n",
    "        title = subplot_group_list[i]['title']\n",
    "        for column_name in subplot_group_list[i]['columns']:\n",
    "            fig.append_trace(go.Scatter(\n",
    "                x=df[x],\n",
    "                y=df[column_name], name=column_name\n",
    "            ), row=row, col=1)\n",
    "\n",
    "    fig.update_layout(height=height, width=width, title_text=plot_title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0ed7f-fca1-4a86-90da-c2d4a8b76fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_array_evolution(exp_progress_dict, exp_interval, field, width_plot, height_plot, zmax=None, zmin=None):\n",
    "    matrix_list = [exp_progress_dict[x][field] for x in exp_progress_dict.keys()]\n",
    "    fig = go.Figure(\n",
    "        data=[go.Heatmap(z=matrix_list[0], zmax=zmax, zmin=zmin)],layout=go.Layout(\n",
    "                title=\"Step 0\",\n",
    "                updatemenus=[dict(\n",
    "                    type=\"buttons\",\n",
    "                    buttons=[dict(label=\"Play\",\n",
    "                                  method=\"animate\",\n",
    "                                  args=[None]),\n",
    "                            dict(label=\"Pause\",\n",
    "                                 method=\"animate\",\n",
    "                                 args=[None,\n",
    "                                       {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                        \"mode\": \"immediate\",\n",
    "                                        \"transition\": {\"duration\": 0}}],\n",
    "                                 )])]\n",
    "            ),\n",
    "            frames=[go.Frame(data=[go.Heatmap(z=matrix_list[i])], \n",
    "                             layout=go.Layout(title_text=f\"Step {i * exp_interval}\")) for i in range(1, len(matrix_list))]\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.update_layout({\n",
    "        'height': height_plot,\n",
    "        'width': width_plot}\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46258b2-b13c-41dd-9e5a-5ca28d90076c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resume here with experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357ddf9-2eb9-4c27-aa7e-856c90a0544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting max/min\n",
    "zmax_plot = 0.015\n",
    "zmin_plot = 0\n",
    "width_plot = 400\n",
    "height_plot = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659ce2e-da1b-42a0-84d0-1ae7e6edf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment factors\n",
    "exp_env_control = None\n",
    "exp_data_path = '../../../rl_agent/simulation_data/data/corrected_motors/run_5_kitchen_no_lights.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59270191-0081-447e-b68c-80c6e3d5f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set runtime values\n",
    "steps = 100000\n",
    "interval = 1000\n",
    "\n",
    "# Agent factors\n",
    "exp_step_size = 0.1\n",
    "exp_epsilon = 0.05\n",
    "exp_chi = 1.1\n",
    "exp_initialization_value = 0.02\n",
    "rolling_avg_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a67299-2264-4a6e-bab9-1fc64e0da7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_env = SolarEnv(reward_data_path=exp_data_path, shape=(37,37), control=exp_env_control)\n",
    "experiment_agent = SolarAgent(step_size=exp_step_size, epsilon=exp_epsilon, \n",
    "                              chi=exp_chi, initialization_value=exp_initialization_value, \n",
    "                              env=experiment_env, rolling_window=rolling_avg_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf2922-97d1-475b-8339-96c152178b4c",
   "metadata": {},
   "source": [
    "#### Initiate an experiment\n",
    "Remember to re-initialize the agent above or else it will resume learning on the exisitng agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56169d-6131-46c9-b9b3-a790b79bd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment_agent, progress_dict, progress_df = run_agent_experiment(exp_agent=experiment_agent, \n",
    "                                                                   exp_env=experiment_env, steps=steps, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7d3ee-fae0-46bb-9cae-858cc6f914fb",
   "metadata": {},
   "source": [
    "#### Show the agent difference from known optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e089d-ba49-432f-a365-8032603f1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_power(progress_df, experiment_env, height=600, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d1857-2d03-4bb9-adf8-6e9b43e14862",
   "metadata": {},
   "source": [
    "#### Visualize state visits and learned values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ea419-aa62-4162-9975-ac1f318c2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_max_step = True\n",
    "specific_step = '100000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8150cc-903e-4eba-bd7e-6d86af1c8312",
   "metadata": {},
   "source": [
    "State visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca816f-012b-4e4c-8c7f-4f0b0417b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_visits'], width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ff4a1-fa21-475c-86c1-0ca6c6b971e9",
   "metadata": {},
   "source": [
    "State values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0630444-7a92-405c-9acc-6d8dab7e2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(progress_dict[(specific_step if not do_max_step else str(steps))]['state_value'], width=width_plot, \n",
    "          height=height_plot, zmin=zmin_plot, zmax=zmax_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0945d39-f673-4897-97c2-aa2fdb125d2a",
   "metadata": {},
   "source": [
    "#### Environment True Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c2d0f-778b-4b03-9983-0d22fadb977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(experiment_env.get_reward_array(), width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eaa99-d91e-4ca2-a5cc-1c0e5470661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_visits', width_plot, height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ed37e-28a0-472b-a036-714c7dc1a198",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visulaize evolution of learned state value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6398b-6be2-4f06-839c-d426b342c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array_evolution(progress_dict, interval, 'state_value', width_plot, height_plot, zmax_plot, zmin_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3221bf-843c-4766-b827-1682a3bf7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE vs steps\n",
    "px.scatter(progress_df, x='step', y='rolling_power', width=width_plot, height=height_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72827c36-139e-4032-8eab-65d772cb6d04",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff4000-d540-4893-a785-11236d03159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request codes\n",
    "MOTOR_CONTROL = 1000\n",
    "STATE_REQUEST = 2000\n",
    "RESET_CODE = 6666\n",
    "\n",
    "def scan_space(arduino):\n",
    "    # Run start\n",
    "    run_start = time.time()\n",
    "    data_dict_list = []\n",
    "    last_motor_interval = 0\n",
    "    last_measure_interval = -1\n",
    "    motor_frequency = 2\n",
    "    measure_frequency = 1\n",
    "    # Set timeouts\n",
    "    abort = False\n",
    "    \n",
    "    for xy_degree in range(0, 181, 5):\n",
    "        for yz_degree in range(0, 181, 5):\n",
    "            si.write_serial_line(arduino, [MOTOR_CONTROL, xy_degree, yz_degree], print_message=False)\n",
    "            new_message, abort = si.listen_for_serial(arduino)\n",
    "            if new_message is not None and not abort:\n",
    "                data_dict_list.append(new_message)\n",
    "            elif abort:\n",
    "                break\n",
    "            else:\n",
    "                print('Empty message received without abort issue')\n",
    "            time.sleep(0.1) # Wait for steady state\n",
    "        if abort:\n",
    "            break\n",
    "        print('xy:',xy_degree,'yz:',yz_degree)\n",
    "    # Write back to start state\n",
    "    write_serial_line(arduino, [si.MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    return pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07994661-9e43-4372-b543-b2641c5cad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('\\nARDUINO CONTROL TESTING')\n",
    "    print('-------------------------')\n",
    "    # Initialize serial port\n",
    "    print('\\nIniitalizing device...')\n",
    "    serial_port = '/dev/cu.usbmodem14101'\n",
    "    baud_rate = 9600\n",
    "    timeout = 5\n",
    "    arduino = si.initialize_serial(serial_port=serial_port, baud_rate=baud_rate, timeout=timeout)\n",
    "    print('\\t - SUCCESS: Device initialized.')\n",
    "    \n",
    "    si.write_serial_line(arduino, [MOTOR_CONTROL, 90, 90])\n",
    "\n",
    "    # Run a loop where motor position incremented every 5 seconds, print out message\n",
    "    print('\\nBeginning loop sequence...')\n",
    "#     data = scan_space(arduino)\n",
    "    print('\\t - Loop complete.')\n",
    "\n",
    "    # Add relative time to returned data and print out\n",
    "#     data['t_relative'] = data['timestamp'] - data['timestamp'].iloc[0]\n",
    "    print('\\nData broadcasted by Arduino:\\n')\n",
    "    \n",
    "#     data.to_csv('/Users/jackogrady/Git/rl-solar/rl_agent/simulation_data/data/run_6_kitchen_no_lights_swapped_motors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55694cd-2430-4233-943d-51758fc16cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1b648-e1a8-4d7d-84ca-56d3c3b878eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_serial_line(arduino, [1000, 180, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e08c6-02c9-4216-80d9-4968f08b292b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
